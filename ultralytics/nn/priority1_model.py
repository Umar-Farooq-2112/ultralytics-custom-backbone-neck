# Ultralytics ðŸš€ AGPL-3.0 License - https://ultralytics.com/license
"""Priority 1: CSPResNet backbone with P2 detection and ECA attention."""

import torch
import torch.nn as nn

from ultralytics.nn.modules import Conv, C2f, SPPF, Detect
from ultralytics.utils import LOGGER


class ECAAttention(nn.Module):
    """Efficient Channel Attention (ECA) - lightweight attention mechanism."""
    
    def __init__(self, channels, gamma=2, b=1):
        """Initialize ECA attention.
        
        Args:
            channels (int): Number of input channels
            gamma (int): Kernel size calculation parameter
            b (int): Kernel size calculation parameter
        """
        super().__init__()
        # Adaptive kernel size calculation
        t = int(abs((torch.log2(torch.tensor(channels, dtype=torch.float32)) + b) / gamma))
        k_size = t if t % 2 else t + 1
        
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.conv = nn.Conv1d(1, 1, kernel_size=k_size, padding=(k_size - 1) // 2, bias=False)
        self.sigmoid = nn.Sigmoid()
    
    def forward(self, x):
        """Forward pass through ECA attention.
        
        Args:
            x (torch.Tensor): Input tensor
            
        Returns:
            torch.Tensor: Attention-weighted output
        """
        # Global average pooling
        y = self.avg_pool(x)
        
        # 1D convolution across channels
        y = self.conv(y.squeeze(-1).transpose(-1, -2)).transpose(-1, -2).unsqueeze(-1)
        
        # Sigmoid activation
        y = self.sigmoid(y)
        
        return x * y.expand_as(x)


class CSPResNetBackbone(nn.Module):
    """CSPResNet backbone with P2, P3, P4, P5 outputs and ECA attention.
    
    This replaces MobileNetV3 with a stronger feature extractor optimized for
    timber defect detection with small objects.
    """
    
    def __init__(self, pretrained=False):
        """Initialize CSPResNet backbone.
        
        Args:
            pretrained (bool): Use pretrained weights (not implemented for custom architecture)
        """
        super().__init__()
        
        # Stem: Input -> 32 -> 64 channels
        self.stem = nn.Sequential(
            Conv(3, 32, k=3, s=2, p=1),  # 640 -> 320
            Conv(32, 64, k=3, s=2, p=1),  # 320 -> 160
        )
        
        # P2 stage (160x160 feature map, stride=4) - For small defects
        self.stage_p2 = nn.Sequential(
            C2f(64, 64, n=1, shortcut=False),
            ECAAttention(64),
        )
        
        # P3 stage (80x80 feature map, stride=8)
        self.downsample_p3 = Conv(64, 128, k=3, s=2, p=1)  # 160 -> 80
        self.stage_p3 = nn.Sequential(
            C2f(128, 128, n=2, shortcut=False),
            ECAAttention(128),
        )
        
        # P4 stage (40x40 feature map, stride=16)
        self.downsample_p4 = Conv(128, 256, k=3, s=2, p=1)  # 80 -> 40
        self.stage_p4 = nn.Sequential(
            C2f(256, 256, n=2, shortcut=False),
            ECAAttention(256),
        )
        
        # P5 stage (20x20 feature map, stride=32)
        self.downsample_p5 = Conv(256, 384, k=3, s=2, p=1)  # 40 -> 20
        self.stage_p5 = nn.Sequential(
            C2f(384, 384, n=1, shortcut=False),
            SPPF(384, 384, k=5),  # SPPF for multi-scale context
            ECAAttention(384),
        )
        
        # Output channels for each pyramid level
        self.out_channels = [64, 128, 256, 384]  # [P2, P3, P4, P5]
    
    def forward(self, x):
        """Forward pass through CSPResNet backbone.
        
        Args:
            x (torch.Tensor): Input image tensor [B, 3, 640, 640]
            
        Returns:
            list: Multi-scale features [P2, P3, P4, P5]
                - P2: [B, 64, 160, 160] stride=4
                - P3: [B, 128, 80, 80] stride=8
                - P4: [B, 256, 40, 40] stride=16
                - P5: [B, 384, 20, 20] stride=32
        """
        # Stem
        x = self.stem(x)  # [B, 64, 160, 160]
        
        # P2 stage
        p2 = self.stage_p2(x)  # [B, 64, 160, 160]
        
        # P3 stage
        x = self.downsample_p3(p2)
        p3 = self.stage_p3(x)  # [B, 128, 80, 80]
        
        # P4 stage
        x = self.downsample_p4(p3)
        p4 = self.stage_p4(x)  # [B, 256, 40, 40]
        
        # P5 stage
        x = self.downsample_p5(p4)
        p5 = self.stage_p5(x)  # [B, 384, 20, 20]
        
        return [p2, p3, p4, p5]


class YOLONeckP2Enhanced(nn.Module):
    """Enhanced YOLO neck with P2 support (4-scale FPN+PAN).
    
    Processes P2, P3, P4, P5 from backbone into detection-ready features.
    """
    
    def __init__(self, in_channels=[64, 128, 256, 384]):
        """Initialize enhanced P2 neck.
        
        Args:
            in_channels (list): Input channels for [P2, P3, P4, P5]
        """
        super().__init__()
        c2, c3, c4, c5 = in_channels
        
        # Top-down pathway (FPN) - Start from P5
        self.reduce_p5 = Conv(c5, 128, k=1, s=1)
        self.c2f_p4_fpn = C2f(c4 + 128, 128, n=2, shortcut=False)
        
        self.reduce_p4 = Conv(128, 96, k=1, s=1)
        self.c2f_p3_fpn = C2f(c3 + 96, 96, n=2, shortcut=False)
        
        self.reduce_p3 = Conv(96, 64, k=1, s=1)
        self.c2f_p2_fpn = C2f(c2 + 64, 64, n=1, shortcut=False)
        
        # Bottom-up pathway (PAN) - Start from P2
        self.downsample_p2 = Conv(64, 64, k=3, s=2, p=1)
        self.c2f_p3_pan = C2f(64 + 96, 96, n=2, shortcut=False)
        
        self.downsample_p3 = Conv(96, 96, k=3, s=2, p=1)
        self.c2f_p4_pan = C2f(96 + 128, 128, n=2, shortcut=False)
        
        self.downsample_p4 = Conv(128, 128, k=3, s=2, p=1)
        self.c2f_p5_pan = C2f(128 + c5, 160, n=2, shortcut=False)
        
        # Output channels for each scale
        self.out_channels = [64, 96, 128, 160]  # [P2, P3, P4, P5]
    
    def forward(self, feats):
        """Forward pass through P2-enhanced neck.
        
        Args:
            feats (list): Multi-scale features [P2, P3, P4, P5]
            
        Returns:
            list: Enhanced features [P2_out, P3_out, P4_out, P5_out]
        """
        p2, p3, p4, p5 = feats
        
        # Top-down pathway (FPN)
        # P5 -> P4
        p5_reduce = self.reduce_p5(p5)
        p5_up = nn.functional.interpolate(p5_reduce, size=p4.shape[-2:], mode='nearest')
        p4_fpn = self.c2f_p4_fpn(torch.cat([p4, p5_up], dim=1))
        
        # P4 -> P3
        p4_reduce = self.reduce_p4(p4_fpn)
        p4_up = nn.functional.interpolate(p4_reduce, size=p3.shape[-2:], mode='nearest')
        p3_fpn = self.c2f_p3_fpn(torch.cat([p3, p4_up], dim=1))
        
        # P3 -> P2
        p3_reduce = self.reduce_p3(p3_fpn)
        p3_up = nn.functional.interpolate(p3_reduce, size=p2.shape[-2:], mode='nearest')
        p2_out = self.c2f_p2_fpn(torch.cat([p2, p3_up], dim=1))
        
        # Bottom-up pathway (PAN)
        # P2 -> P3
        p2_down = self.downsample_p2(p2_out)
        p3_out = self.c2f_p3_pan(torch.cat([p2_down, p3_fpn], dim=1))
        
        # P3 -> P4
        p3_down = self.downsample_p3(p3_out)
        p4_out = self.c2f_p4_pan(torch.cat([p3_down, p4_fpn], dim=1))
        
        # P4 -> P5
        p4_down = self.downsample_p4(p4_out)
        p5_out = self.c2f_p5_pan(torch.cat([p4_down, p5], dim=1))
        
        return [p2_out, p3_out, p4_out, p5_out]


class EnhancedDetectHead(nn.Module):
    """Enhanced detection head with P2 support (4-scale detection)."""
    
    def __init__(self, nc, ch):
        """Initialize enhanced detection head.
        
        Args:
            nc (int): Number of classes
            ch (tuple): Input channels for each detection level [P2, P3, P4, P5]
        """
        super().__init__()
        
        # Single refinement layer before detection for each scale
        self.pre_detect_p2 = Conv(ch[0], ch[0], k=3, s=1)
        self.pre_detect_p3 = Conv(ch[1], ch[1], k=3, s=1)
        self.pre_detect_p4 = Conv(ch[2], ch[2], k=3, s=1)
        self.pre_detect_p5 = Conv(ch[3], ch[3], k=3, s=1)
        
        # Standard YOLO detect head with 4 scales
        self.detect = Detect(nc=nc, ch=ch)
        
    def forward(self, x):
        """Forward pass through enhanced detection head.
        
        Args:
            x (list): Feature maps from neck [P2, P3, P4, P5]
            
        Returns:
            Detection output
        """
        # Refine features before detection
        x2 = self.pre_detect_p2(x[0])
        x3 = self.pre_detect_p3(x[1])
        x4 = self.pre_detect_p4(x[2])
        x5 = self.pre_detect_p5(x[3])
        
        # Detection
        return self.detect([x2, x3, x4, x5])


class MobileNetV3YOLO(nn.Module):
    """Priority 1: Custom YOLO model with CSPResNet backbone for timber defect detection.
    
    This model implements Priority 1 enhancements:
    - CSPResNet backbone with ECA attention (better feature extraction)
    - P2-enhanced neck with FPN+PAN (4-scale detection)
    - Enhanced detection head (4 detection scales: P2, P3, P4, P5)
    
    Optimized for timber defect detection:
    - P2 (160x160): Small defects like cracks
    - P3 (80x80): Medium defects like small knots
    - P4 (40x40): Large defects like big knots
    - P5 (20x20): Very large defects and context
    
    Expected performance: ~85% mAP (baseline: 80%)
    Parameter target: 4-6M (actual: 5.22M)
    """
    
    def __init__(self, nc=80, pretrained=True, verbose=True):
        """Initialize Priority 1 model.
        
        Args:
            nc (int): Number of classes
            pretrained (bool): Use pretrained backbone (not used for custom CSPResNet)
            verbose (bool): Print model information
        """
        super().__init__()
        self.nc = nc
        self.task = 'detect'
        self.names = {i: f"{i}" for i in range(nc)}
        
        # Priority 1: CSPResNet backbone with ECA attention
        self.backbone = CSPResNetBackbone(pretrained=False)
        
        # Priority 1: P2-enhanced neck
        self.neck = YOLONeckP2Enhanced(in_channels=self.backbone.out_channels)
        
        # Neck output channels: [64, 96, 128, 160] for [P2, P3, P4, P5]
        neck_out_channels = self.neck.out_channels
        
        # Enhanced detection head with 4 scales
        self.head = EnhancedDetectHead(nc=nc, ch=tuple(neck_out_channels))
        
        # Create model list for compatibility with v8DetectionLoss
        self.model = nn.ModuleList([self.backbone, self.neck, self.head.detect])
        
        # Model metadata
        self.stride = torch.tensor([4, 8, 16, 32])  # P2, P3, P4, P5 strides
        self.yaml = {'nc': nc, 'custom_model': 'cspresnet-yolo-p2'}
        self.args = {}
        self.pt_path = None
        
        # Initialize head
        self._initialize_head()
        
        if verbose:
            self.info()
    
    def _initialize_head(self):
        """Initialize detection head with proper strides."""
        m = self.head.detect
        if isinstance(m, Detect):
            s = 640
            m.inplace = True
            
            # Forward pass to get strides
            forward = lambda x: self.forward(x)
            m.stride = torch.tensor([s / x.shape[-2] for x in forward(torch.zeros(1, 3, s, s))])
            self.stride = m.stride
            m.bias_init()
    
    def forward(self, x, *args, **kwargs):
        """Forward pass through the model.
        
        Args:
            x (torch.Tensor | dict): Input tensor for inference or batch dict for training
            
        Returns:
            Detection outputs or (loss, loss_items) for training
        """
        # Training mode - if input is dict, compute loss
        if isinstance(x, dict):
            return self.loss(x, *args, **kwargs)
        
        # Inference mode
        feats = self.backbone(x)  # [P2, P3, P4, P5]
        fused_feats = self.neck(feats)  # [P2, P3, P4, P5] enhanced
        outputs = self.head(fused_feats)
        
        return outputs
    
    def loss(self, batch, preds=None):
        """Compute loss.
        
        Args:
            batch (dict): Batch to compute loss on
            preds: Predictions (optional)
            
        Returns:
            (tuple): (total_loss, loss_items)
        """
        if not hasattr(self, 'criterion') or self.criterion is None:
            self.criterion = self.init_criterion()
        
        if preds is None:
            preds = self.forward(batch["img"])
        return self.criterion(preds, batch)
    
    def init_criterion(self):
        """Initialize the loss criterion."""
        from ultralytics.utils.loss import v8DetectionLoss
        return v8DetectionLoss(self)
    
    def fuse(self, verbose=True):
        """Fuse Conv2d + BatchNorm2d layers."""
        if not self.is_fused():
            for m in self.modules():
                if isinstance(m, Conv) and hasattr(m, 'bn'):
                    from ultralytics.utils.torch_utils import fuse_conv_and_bn
                    m.conv = fuse_conv_and_bn(m.conv, m.bn)
                    delattr(m, 'bn')
                    m.forward = m.forward_fuse
            if verbose:
                self.info()
        return self
    
    def is_fused(self, thresh=10):
        """Check if model has less than threshold BatchNorm layers."""
        bn = tuple(v for k, v in nn.__dict__.items() if 'Norm' in k)
        return sum(isinstance(v, bn) for v in self.modules()) < thresh
    
    def info(self, detailed=False, verbose=True, imgsz=640):
        """Print model information."""
        from ultralytics.utils.torch_utils import model_info
        return model_info(self, detailed=detailed, verbose=verbose, imgsz=imgsz)
    
    def predict(self, x, profile=False, visualize=False, augment=False):
        """Perform inference."""
        if augment:
            y = []
            for xi in [x, torch.flip(x, [-1])]:
                yi = self.forward(xi)
                y.append(yi)
            return torch.cat(y, -1)
        return self.forward(x)
    
    def load(self, weights):
        """Load weights from checkpoint."""
        if isinstance(weights, str):
            ckpt = torch.load(weights, map_location='cpu')
            if isinstance(ckpt, dict):
                state_dict = ckpt.get('model', ckpt)
                if hasattr(state_dict, 'state_dict'):
                    state_dict = state_dict.state_dict()
            else:
                state_dict = ckpt
            self.load_state_dict(state_dict, strict=False)
        else:
            self.load_state_dict(weights, strict=False)
